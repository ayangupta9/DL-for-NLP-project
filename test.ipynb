{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from base_bert import BertPreTrainedModel\n",
    "from utils import get_extended_attention_mask\n",
    "from bert import BertModel\n",
    "from config import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (word_embedding): Embedding(30522, 768, padding_idx=0)\n",
       "  (pos_embedding): Embedding(512, 768)\n",
       "  (tk_type_embedding): Embedding(2, 768)\n",
       "  (embed_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (embed_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (bert_layers): ModuleList(\n",
       "    (0-11): 12 x BertLayer(\n",
       "      (self_attention): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (attention_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (interm_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (out_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (out_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (out_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (pooler_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pooler_af): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # initialize the linear transformation layers for key, value, query\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        # this dropout is applied to normalized attention scores following the original implementation of transformer\n",
    "        # although it is a bit unusual, we empirically observe that it yields better performance\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transform(self, x, linear_layer):\n",
    "        # the corresponding linear_layer of k, v, q are used to project the hidden_state (x)\n",
    "        bs, seq_len = x.shape[:2]\n",
    "        proj = linear_layer(x)\n",
    "        # next, we need to produce multiple heads for the proj\n",
    "        # this is done by spliting the hidden state to self.num_attention_heads, each of size self.attention_head_size\n",
    "        proj = proj.view(bs, seq_len, self.num_attention_heads, self.attention_head_size)\n",
    "        # by proper transpose, we have proj of [bs, num_attention_heads, seq_len, attention_head_size]\n",
    "        proj = proj.transpose(1, 2)\n",
    "        return proj\n",
    "\n",
    "    def attention(self, key, query, value, attention_mask):\n",
    "        # each attention is calculated following eq (1) of https://arxiv.org/pdf/1706.03762.pdf.\n",
    "        # attention scores are calculated by multiplying queries and keys\n",
    "        # and get back a score matrix S of [bs, num_attention_heads, seq_len, seq_len]\n",
    "        # S[*, i, j, k] represents the (unnormalized) attention score between the j-th\n",
    "        # and k-th token, given by i-th attention head before normalizing the scores,\n",
    "        # use the attention mask to mask out the padding token scores.\n",
    "\n",
    "        # Note again: in the attention_mask non-padding tokens are marked with 0 and\n",
    "        # adding tokens with a large negative number.\n",
    "\n",
    "        ### TODO\n",
    "        q_kt = torch.dot(query, key) \n",
    "        # / torch.sqrt(key.shape)\n",
    "        # raise NotImplementedError\n",
    "        # Normalize the scores.\n",
    "        # Multiply the attention scores to the value and get back V'.\n",
    "        # Next, we need to concat multi-heads and recover the original shape\n",
    "        # [bs, seq_len, num_attention_heads * attention_head_size = hidden_size].\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        \"\"\"\n",
    "        hidden_states: [bs, seq_len, hidden_state]\n",
    "        attention_mask: [bs, 1, 1, seq_len]\n",
    "        output: [bs, seq_len, hidden_state]\n",
    "        \"\"\"\n",
    "        # first, we have to generate the key, value, query for each token for multi-head attention w/ transform (more details inside the function)\n",
    "        # of *_layers are of [bs, num_attention_heads, seq_len, attention_head_size]\n",
    "        key_layer = self.transform(hidden_states, self.key)\n",
    "        value_layer = self.transform(hidden_states, self.value)\n",
    "        query_layer = self.transform(hidden_states, self.query)\n",
    "        # calculate the multi-head attention\n",
    "        attn_value = self.attention(key_layer, query_layer, value_layer, attention_mask)\n",
    "        return attn_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "config = {\n",
    "        \"hidden_dropout_prob\": 0.3,\n",
    "        \"hidden_size\": 768,\n",
    "        \"data_dir\": \".\",\n",
    "        # \"option\": \"pretrain\",\n",
    "        \"local_files_only\": True,\n",
    "}\n",
    "\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()\n",
    "config.name_or_path = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'architectures': None, 'finetuning_task': None, 'id2label': None, 'label2id': None, 'num_labels': 2, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'xla_device': None, '_name_or_path': '', 'vocab_size': 30522, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 2, 'initializer_range': 0.02, 'layer_norm_eps': 1e-12, 'gradient_checkpointing': False, 'position_embedding_type': 'absolute', 'use_cache': True, 'name_or_path': 'bert-base-uncased'}\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"My name is Ayan Gupta!\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer(sentence, padding='max_length', max_length=10, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from input IDs: ['[CLS]', 'my', 'name', 'is', 'a', '##yan', 'gupta', '!', '[SEP]', '[PAD]']\n",
      "Words from input IDs: [CLS] my name is ayan gupta ! [SEP] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# tokens_from_ids = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "# print(\"Tokens from input IDs:\", tokens_from_ids)\n",
    "\n",
    "# words_from_ids = tokenizer.convert_tokens_to_string(tokens_from_ids)\n",
    "# print(\"Words from input IDs:\", words_from_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]),\n",
      " 'input_ids': tensor([[  101,  2026,  2171,  2003,  1037,  7054, 20512,   999,   102,     0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
